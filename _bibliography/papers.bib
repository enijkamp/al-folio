---
---

@article{nijkamp2020learning,
  title={Learning Energy-based Model with Flow-based Backbone by Neural Transport MCMC},
  author={Nijkamp, Erik and Gao, Ruiqi and Sountsov, Pavel and Vasudevan, Srinivas and Pang, Bo and Zhu, Song-Chun and Wu, Ying Nian},
  journal={arXiv preprint arXiv:2006.06897},
  year={2020},
  abbr={arXiv},
  abstract={Learning energy-based model (EBM) requires MCMC sampling of the learned model as the inner loop of the learning algorithm. However, MCMC sampling of EBM in data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both the theory and practice of EBM. In this paper, we propose to learn EBM with a flow-based model serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the flow-based model, and MCMC sampling of the EBM in the latent space, which is a simple special case of neural transport MCMC, mixes well and traverses modes in the data space. This enables proper sampling and learning of EBM.},
  pdf={https://arxiv.org/pdf/2006.06897},
  selected = true
}

@article{nijkamp2020learning,
  title={Learning Multi-layer Latent Variable Model via Variational Optimization of Short Run MCMC for Approximate Inference},
  author={Nijkamp, Erik and Pang, Bo and Han, Tian and Zhou, Linqi and Zhu, Song-Chun and Wu, Ying Nian},
  journal={European Conference on Computer Vision (ECCV)},
  year={2020},
  abbr={ECCV},
  award={Spotlight},
  abstract={This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.},
  pdf={https://arxiv.org/pdf/1912.01909},
  code={https://github.com/enijkamp/short_run_inf},
  selected = true
}

@article{nijkamp2019anatomy,
  title={On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models},
  author={Nijkamp, Erik and Hill, Mitch and Han, Tian and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={2020},
  abbr={AAAI},
  award={Oral},
  abstract={This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling.
On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training.
On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.},
  pdf={https://arxiv.org/pdf/1903.12370.pdf},
  website={https://enijkamp.github.io/project_anatomy/},
  code={https://github.com/point0bar1/ebm-anatomy},
  poster={https://github.com/enijkamp/project_anatomy/blob/master/ebm-anatomy-poster.pdf},
  selected = true
}

@article{pang2020learning,
  title={Learning Latent Space Energy-Based Prior Model},
  author={Pang, Bo* and Han, Tian* and Nijkamp, Erik* and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  abbr={NeurIPS},
  abstract={The generator model assumes that the observed example is generated by a low-dimensional latent vector via a top-down network, and the latent vector follows a simple and known prior distribution, such as uniform or Gaussian white noise distribution. While we can learn an expressive top-down network to map the prior distribution to the data distribution, we can also learn an expressive prior model instead of assuming a given prior distribution. This follows the philosophy of empirical Bayes where the prior model is learned from the observed data. We propose to learn an energy-based prior model for the latent vector, where the energy function is parametrized by a very simple multi-layer perceptron. Due to the low-dimensionality of the latent space, learning a latent space energy-based prior model proves to be both feasible and desirable. In this paper, we develop the maximum likelihood learning algorithm and its variation based on short-run Markov chain Monte Carlo sampling from the prior and the posterior distributions of the latent vector, and we show that the learned model exhibits strong performance in terms of image and text generation and anomaly detection.},
  pdf={https://arxiv.org/pdf/2006.08205.pdf},
  code={https://github.com/bpucla/latent-space-EBM-prior},
  selected = true
}

@article{gao2020flow,
  title={Flow Contrastive Estimation of Energy-Based Models},
  author={Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P and Xu, Zhen and Dai, Andrew M and Wu, Ying Nian},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7518--7528},
  year={2020},
  abbr={CVPR},
  abstract={This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.},
  pdf={https://arxiv.org/pdf/1912.00589.pdf},
  website={http://www.stat.ucla.edu/~ruiqigao/fce/main.html},
  award={Oral},
  selected = true
}

@article{han2020joint,
  title={Joint Training of Variational Auto-Encoder and Latent Energy-Based Model},
  author={Han, Tian and Nijkamp, Erik and Zhou, Linqi and Pang, Bo and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7978--7987},
  year={2020},
  abbr={CVPR},
  abstract={This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.},
  pdf={https://arxiv.org/pdf/1812.10907.pdf},
  website={https://hthth0801.github.io/jointLearning/},
  code={https://github.com/alexzhou907/vae_ebm},
  selected = true
}

@article{han2019divergence,
  title={Divergence Triangle for Joint Training of Generator model, Energy-based model, and Inferential model},
  author={Han, Tian* and Nijkamp, Erik* and Fang, Xiaolin and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={8670--8679},
  year={2019},
  abbr={CVPR},
  abstract={This paper proposes the divergence triangle as a framework for joint training of generator model, energy-based model and inference model. The divergence triangle is a compact and symmetric (anti-symmetric) objective function that seamlessly integrates variational learning, adversarial learning, wake-sleep algorithm, and contrastive divergence in a unified probabilistic formulation. This unification makes the processes of sampling, inference, energy evaluation readily available without the need for costly Markov chain Monte Carlo methods. Our experiments demonstrate that the divergence triangle is capable of learning (1) an energy-based model with well-formed energy landscape, (2) direct sampling in the form of a generator network, and (3) feed-forward inference that faithfully reconstructs observed as well as synthesized data. The divergence triangle is a robust training method that can learn from incomplete data.},
  pdf={https://arxiv.org/pdf/1812.10907.pdf},
  website={https://enijkamp.github.io/project_triangle/},
  code={http://github.com/enijkamp/triangle},
  poster={https://raw.githubusercontent.com/enijkamp/project_triangle/master/cvpr2019_tianhan.pdf},
  award={Oral},
  selected = true
}

@article{nijkamp2019learning,
  title={Learning Non-convergent Non-persistent Short-run MCMC toward Energy-Based Model},
  author={Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5232--5242},
  year={2019},
  abbr={NeurIPS},
  abstract={This paper studies a curious phenomenon in learning energy-based model (EBM) using MCMC. In each learning iteration, we generate synthesized examples by running a non-convergent, non-mixing, and non-persistent short-run MCMC toward the current model, always starting from the same initial distribution such as uniform noise distribution, and always running a fixed number of MCMC steps. After generating synthesized examples, we then update the model parameters according to the maximum likelihood learning gradient, as if the synthesized examples are fair samples from the current model. We treat this non-convergent short-run MCMC as a learned generator model or a flow model. We provide arguments for treating the learned non-convergent short-run MCMC as a valid model. We show that the learned short-run MCMC is capable of generating realistic images. More interestingly, unlike traditional EBM or MCMC, the learned short-run MCMC is capable of reconstructing observed images and interpolating between images, like generator or flow models. The code can be found in the Appendix.},
  pdf={https://arxiv.org/pdf/1904.09770},
  website={https://enijkamp.github.io/project_short_run},
  poster={https://github.com/enijkamp/project_short_run/blob/master/poster.pdf},
  code={https://github.com/enijkamp/short_run},
  selected = true
}

@article{hill2019building,
  title={Building a telescope to look into high-dimensional image spaces},
  author={Hill, Mitch and Nijkamp, Erik and Zhu, Song-Chun},
  journal={Quarterly of Applied Mathematics},
  year={2019},
  abstract={An image pattern can be represented by a probability distribution whose density is concentrated on different low-dimensional subspaces in the high-dimensional image space. Such probability densities have an astronomical number of local modes corresponding to typical pattern appearances. Related groups of modes can join to form macroscopic image basins that represent pattern concepts. Recent works use neural networks that capture high-order image statistics to learn Gibbs models capable of synthesizing realistic images of many patterns. However, characterizing a learned probability density to uncover the Hopfield memories of the model, encoded by the structure of the local modes, remains an open challenge. In this work, we present novel computational experiments that map and visualize the local mode structure of Gibbs densities. Efficient mapping requires identifying the global basins without enumerating the countless modes. Inspired by Grenander's jump-diffusion method, we propose a new MCMC tool called Attraction-Diffusion (AD) that can capture the macroscopic structure of highly non-convex densities by measuring metastability of local modes. AD involves altering the target density with a magnetization potential penalizing distance from a known mode and running an MCMC sample of the altered density to measure the stability of the initial chain state. Using a low-dimensional generator network to facilitate exploration, we map image spaces with up to 12,288 dimensions (64 Ã— 64 pixels in RGB). Our work shows: (1) AD can efficiently map highly non-convex probability densities, (2) metastable regions of pattern probability densities contain coherent groups of images, and (3) the perceptibility of differences between training images influences the metastability of image basins.},
  abbr={QAM},
  pdf={https://arxiv.org/pdf/1803.01043},
}

@article{pang2020towards,
  title={Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation},
  author={Pang, Bo and Nijkamp, Erik and Han, Wenjuan and Zhou, Linqi and Liu, Yixian and Tu, Kewei},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3619--3629},
  year={2020},
  abbr={ACL},
  abstract={Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.},
  pdf={https://www.aclweb.org/anthology/2020.acl-main.333.pdf}
}

@article{deutsch2019generative,
  title={A generative model for sampling high-performance and diverse weights for neural networks},
  author={Deutsch, Lior and Nijkamp, Erik and Yang, Yu},
  journal={arXiv preprint arXiv:1905.02898},
  year={2019},
  pdf={https://arxiv.org/pdf/1905.02898},
  abbr={arXiv},
  abstract={Recent work on mode connectivity in the loss landscape of deep neural networks has demonstrated that the locus of (sub-)optimal weight vectors lies on continuous paths. In this work, we train a neural network that serves as a hypernetwork, mapping a latent vector into high-performance (low-loss) weight vectors, generalizing recent findings of mode connectivity to higher dimensional manifolds. We formulate the training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We demonstrate how to reduce the number of parameters in the hypernetwork by parameter sharing. Once learned, the hypernetwork allows for a computationally efficient, ancestral sampling of neural network weights, which we recruit to form large ensembles. The improvement in classification accuracy obtained by this ensembling indicates that the generated manifold extends in dimensions other than directions implied by trivial symmetries. For computational efficiency, we distill an ensemble into a single classifier while retaining generalization.}
}

@inproceedings{gemulla2011large,
  title={Large-scale matrix factorization with distributed stochastic gradient descent},
  author={Gemulla, Rainer and Nijkamp, Erik and Haas, Peter J and Sismanis, Yannis},
  booktitle={Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={69--77},
  year={2011},
  abbr={SIGKDD},
  pdf={https://dl.acm.org/doi/pdf/10.1145/2020408.2020426}
}

@article{hwang2010binrank,
  title={Binrank: Scaling dynamic authority-based search using materialized subgraphs},
  author={Hwang, Heasoo and Balmin, Andrey and Reinwald, Berthold and Nijkamp, Erik},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={22},
  number={8},
  pages={1176--1190},
  year={2010},
  publisher={IEEE}
}

@inproceedings{xie2017generative,
  title={Generative hierarchical learning of sparse FRAME models},
  author={Xie, Jianwen and Xu, Yifei and Nijkamp, Erik and Nian Wu, Ying and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7399--7407},
  year={2017}
}

@article{nijkamp2009embedded,
  title={Embedded analytics in front office applications},
  author={Nijkamp, Erik and Oberhofer, Martin},
  journal={Datenbanksysteme in Business, Technologie und Web (BTW)--13. Fachtagung des GI-Fachbereichs" Datenbanken und Informationssysteme"(DBIS)},
  year={2009},
  publisher={Gesellschaft f{\"u}r Informatik eV}
}

@article{alexandrov2011mapreduce,
  title={MapReduce and PACT-comparing data parallel programming models},
  author={Alexandrov, Alexander and Ewen, Stephan and Heimel, Max and Hueske, Fabian and Kao, Odej and Markl, Volker and Nijkamp, Erik and Warneke, Daniel},
  journal={Datenbanksysteme f{\"u}r Business, Technologie und Web (BTW)},
  year={2011},
  publisher={Gesellschaft f{\"u}r Informatik eV}
}

@phdthesis{nijkamp2018dance,
  title={A Dance with the Langevin Equation},
  author={Nijkamp, Erik},
  year={2018},
  school={UCLA}
}

@article{alexandrov2010massively,
  title={Massively parallel data analysis with pacts on nephele},
  author={Alexandrov, Alexander and Heimel, Max and Markl, Volker and Battr{\'e}, Dominic and Hueske, Fabian and Nijkamp, Erik and Ewen, Stephan and Kao, Odej and Warneke, Daniel},
  journal={Proceedings of the VLDB Endowment},
  volume={3},
  number={1-2},
  pages={1625--1628},
  year={2010},
  publisher={VLDB Endowment}
}

@article{baid2008dbpubs,
  title={Dbpubs: multidimensional exploration of database publications},
  author={Baid, Akanksha and Balmin, Andrey and Hwang, Heasoo and Nijkamp, Erik and Rao, Jun and Reinwald, Berthold and Simitsis, Alkis and Sismanis, Yannis and van Ham, Frank},
  journal={Proceedings of the VLDB Endowment},
  volume={1},
  number={2},
  pages={1456--1459},
  year={2008},
  publisher={VLDB Endowment}
}

@article{xie2020representation,
  title={Representation Learning: A Statistical Perspective},
  author={Xie, Jianwen and Gao, Ruiqi and Nijkamp, Erik and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Annual Review of Statistics and Its Application},
  volume={7},
  pages={303--335},
  year={2020},
  publisher={Annual Reviews},
  pdf={https://arxiv.org/pdf/1911.11374}
}

@article{pang2020deep,
  title={Deep Learning With TensorFlow: A Review},
  author={Pang, Bo and Nijkamp, Erik and Wu, Ying Nian},
  journal={Journal of Educational and Behavioral Statistics},
  volume={45},
  number={2},
  pages={227--248},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA},
  pdf={https://journals.sagepub.com/doi/pdf/10.3102/1076998619872761}
}