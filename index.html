<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Erik Lennart Nijkamp</title>
<meta name="description" content="Research portfolio of Erik Lennart Nijkamp.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%65%6E%69%6A%6B%61%6D%70@%75%63%6C%61.%65%64%75"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=8osGCyAAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/enijkamp" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/enijkamp" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/erik_nijkamp" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <!--  <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
           -->
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Erik</span> Lennart  Nijkamp
    </h1>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/me.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p><strong>About.</strong> I’m a Ph.D. candidate in the <a href="https://vcla.stat.ucla.edu/" target="\_blank">Center for Vision, Cognition, Learning and Autonomy</a> (VCLA) at <a href="http://ucla.edu" target="\_blank">UCLA</a>, advised by Prof. Song-Chun Zhu and Prof. Ying Nian Wu. I’ve also spent time at <a href="https://www.research.ibm.com/labs/almaden/" target="\_blank">IBM Research</a>, <a href="https://research.google/" target="\_blank">Google Research</a>, and <a href="https://einstein.ai/" target="\_blank">Salesforce Einstein Research</a>. My research is generously supported by the UCLA DYF Fellowship, <a href="https://www.xsede.org/" target="\_blank">XSEDE</a> extreme science and engineering grant, and the <a href="https://developer.nvidia.com/academic_gpu_seeding" target="\_blank">NVIDIA GPU grant</a>.</p>

<p><strong>Research interests.</strong> Representation Learning, Generative Models, Unsupervised Learning, Energy Based Models, Variational Approximation, Computer Vision, Natural Language Processing.</p>

<p><strong>Research themes.</strong> The governing theme of our research is to <em>advance and establish energy-based models:</em></p>

<p>(1) Latent space modelling and sampling.</p>
<ul>
  <li>Latent EBM for semi-supervised classification <a href="https://arxiv.org/pdf/2010.09359.pdf" target="\_blank">(Pang &amp; Nijkamp, ICBINB@NeurIPS 2020)</a>.</li>
  <li>Latent EBM as exponential tilted priors <a href="https://arxiv.org/pdf/2006.08205.pdf" target="\_blank">(Pang &amp; Nijkamp et al., NeurIPS 2020)</a>.</li>
</ul>

<p>(2) Variations of MCMC-based learning.</p>
<ul>
  <li>Mixing MCMC <a href="https://arxiv.org/pdf/2006.06897.pdf" target="\_blank">(Preprint 2020, Nijkamp et al.)</a>.</li>
  <li>Short-run MCMC for sampling <a href="https://arxiv.org/pdf/1904.09770.pdf" target="\_blank">(NeurIPS 2019, Nijkamp et al.)</a> and inference <a href="https://arxiv.org/pdf/1912.01909.pdf" target="\_blank">(ECCV 2020, Nijkamp et al.)</a>.</li>
  <li>Anatomy of MCMC-based learning <a href="https://arxiv.org/pdf/1903.12370.pdf" target="\_blank">(AAAI 2020, Nijkamp et al.)</a>.</li>
  <li>Mapping of learned energy potentials <a href="https://arxiv.org/pdf/1803.01043.pdf" target="\_blank">(QAM 2020, Hill &amp; Nijkamp et al.)</a>.</li>
</ul>

<p>(3) Joint training of EBMs without resorting to MCMC.</p>
<ul>
  <li>Learning with Flow-based models <a href="https://arxiv.org/pdf/1912.00589.pdf" target="\_blank">(CVPR 2020, Gao &amp; Nijkamp et al.)</a>.</li>
  <li>Learning with VAE-based models <a href="https://arxiv.org/pdf/1812.10907.pdf" target="\_blank">(CVPR 2019, Han &amp; Nijkamp et al.)</a>, <a href="https://arxiv.org/pdf/1812.10907.pdf" target="\_blank">(CVPR 2020, Han &amp; Nijkamp et al.)</a>.</li>
</ul>

<p><strong>Selected publications.</strong></p>
<div class="publications">
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
      
    
  
  </div>

  <div id="nijkamp2020learning" class="col-sm-8">
    
      <div class="title">Learning Energy-based Model with Flow-based Backbone by Neural Transport MCMC</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Gao, Ruiqi,
                
              
            
          
        
          
            
              
                
                  Sountsov, Pavel,
                
              
            
          
        
          
            
              
                
                  Vasudevan, Srinivas,
                
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2006.06897</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2006.06897" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Learning energy-based model (EBM) requires MCMC sampling of the learned model as the inner loop of the learning algorithm. However, MCMC sampling of EBM in data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both the theory and practice of EBM. In this paper, we propose to learn EBM with a flow-based model serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the flow-based model, and MCMC sampling of the EBM in the latent space, which is a simple special case of neural transport MCMC, mixes well and traverses modes in the data space. This enables proper sampling and learning of EBM.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECCV</abbr>
      
      <span class="award badge">Spotlight</span>
      
    
  
  </div>

  <div id="nijkamp2020learninh" class="col-sm-8">
    
      <div class="title">Learning Multi-layer Latent Variable Model via Variational Optimization of Short Run MCMC for Approximate Inference</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                
                  Zhou, Linqi,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>European Conference on Computer Vision (ECCV)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1912.01909" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/enijkamp/short_run_inf" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://enijkamp.github.io/project_short_run_inference/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="nijkamp2019anatomy" class="col-sm-8">
    
      <div class="title">On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1903.12370.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/point0bar1/ebm-anatomy" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://github.com/enijkamp/project_anatomy/blob/master/ebm-anatomy-poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_anatomy/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling.
On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training.
On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
      
    
  
  </div>

  <div id="pang2020learning" class="col-sm-8">
    
      <div class="title">Learning Latent Space Energy-Based Prior Model</div>
      <div class="author">
        
          
            
              
                
                  Pang, Bo*,
                
              
            
          
        
          
            
              
                
                  Han, Tian*,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik*</em>,
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2006.08205.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/bpucla/latent-space-EBM-prior" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The generator model assumes that the observed example is generated by a low-dimensional latent vector via a top-down network, and the latent vector follows a simple and known prior distribution, such as uniform or Gaussian white noise distribution. While we can learn an expressive top-down network to map the prior distribution to the data distribution, we can also learn an expressive prior model instead of assuming a given prior distribution. This follows the philosophy of empirical Bayes where the prior model is learned from the observed data. We propose to learn an energy-based prior model for the latent vector, where the energy function is parametrized by a very simple multi-layer perceptron. Due to the low-dimensionality of the latent space, learning a latent space energy-based prior model proves to be both feasible and desirable. In this paper, we develop the maximum likelihood learning algorithm and its variation based on short-run Markov chain Monte Carlo sampling from the prior and the posterior distributions of the latent vector, and we show that the learned model exhibits strong performance in terms of image and text generation and anomaly detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="gao2020flow" class="col-sm-8">
    
      <div class="title">Flow Contrastive Estimation of Energy-Based Models</div>
      <div class="author">
        
          
            
              
                
                  Gao, Ruiqi,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Kingma, Diederik P,
                
              
            
          
        
          
            
              
                
                  Xu, Zhen,
                
              
            
          
        
          
            
              
                
                  Dai, Andrew M,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1912.00589.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      <a href="http://www.stat.ucla.edu/ ruiqigao/fce/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
    
  
  </div>

  <div id="han2020joint" class="col-sm-8">
    
      <div class="title">Joint Training of Variational Auto-Encoder and Latent Energy-Based Model</div>
      <div class="author">
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Zhou, Linqi,
                
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1812.10907.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/alexzhou907/vae_ebm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://hthth0801.github.io/jointLearning/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="han2019divergence" class="col-sm-8">
    
      <div class="title">Divergence Triangle for Joint Training of Generator model, Energy-based model, and Inferential model</div>
      <div class="author">
        
          
            
              
                
                  Han, Tian*,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik*</em>,
              
            
          
        
          
            
              
                
                  Fang, Xiaolin,
                
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1812.10907.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="http://github.com/enijkamp/triangle" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://raw.githubusercontent.com/enijkamp/project_triangle/master/cvpr2019_tianhan.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_triangle/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes the divergence triangle as a framework for joint training of generator model, energy-based model and inference model. The divergence triangle is a compact and symmetric (anti-symmetric) objective function that seamlessly integrates variational learning, adversarial learning, wake-sleep algorithm, and contrastive divergence in a unified probabilistic formulation. This unification makes the processes of sampling, inference, energy evaluation readily available without the need for costly Markov chain Monte Carlo methods. Our experiments demonstrate that the divergence triangle is capable of learning (1) an energy-based model with well-formed energy landscape, (2) direct sampling in the form of a generator network, and (3) feed-forward inference that faithfully reconstructs observed as well as synthesized data. The divergence triangle is a robust training method that can learn from incomplete data.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
      
    
  
  </div>

  <div id="nijkamp2019learning" class="col-sm-8">
    
      <div class="title">Learning Non-convergent Non-persistent Short-run MCMC toward Energy-Based Model</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1904.09770" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/enijkamp/short_run" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://github.com/enijkamp/project_short_run/blob/master/poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_short_run" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies a curious phenomenon in learning energy-based model (EBM) using MCMC. In each learning iteration, we generate synthesized examples by running a non-convergent, non-mixing, and non-persistent short-run MCMC toward the current model, always starting from the same initial distribution such as uniform noise distribution, and always running a fixed number of MCMC steps. After generating synthesized examples, we then update the model parameters according to the maximum likelihood learning gradient, as if the synthesized examples are fair samples from the current model. We treat this non-convergent short-run MCMC as a learned generator model or a flow model. We provide arguments for treating the learned non-convergent short-run MCMC as a valid model. We show that the learned short-run MCMC is capable of generating realistic images. More interestingly, unlike traditional EBM or MCMC, the learned short-run MCMC is capable of reconstructing observed images and interpolating between images, like generator or flow models. The code can be found in the Appendix.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    </div>

    

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-180825462-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-180825462-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
