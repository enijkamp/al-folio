<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Erik Lennart Nijkamp | publications</title>
<meta name="description" content="Research portfolio of Erik Lennart Nijkamp.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Erik</span> Lennart  Nijkamp
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <!--  <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
           -->
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <p><em class="star">*</em> denotes equal contribution</p>

<p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=8osGCyAAAAAJ" target="\_blank">Google Scholar</a>.</p>

<div class="publications">


  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
      
    
  
  </div>

  <div id="nijkamp2020learning" class="col-sm-8">
    
      <div class="title">Learning Energy-based Model with Flow-based Backbone by Neural Transport MCMC</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Gao, Ruiqi,
                
              
            
          
        
          
            
              
                
                  Sountsov, Pavel,
                
              
            
          
        
          
            
              
                
                  Vasudevan, Srinivas,
                
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2006.06897</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2006.06897" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Learning energy-based model (EBM) requires MCMC sampling of the learned model as the inner loop of the learning algorithm. However, MCMC sampling of EBM in data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both the theory and practice of EBM. In this paper, we propose to learn EBM with a flow-based model serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the flow-based model, and MCMC sampling of the EBM in the latent space, which is a simple special case of neural transport MCMC, mixes well and traverses modes in the data space. This enables proper sampling and learning of EBM.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECCV</abbr>
      
      <span class="award badge">Spotlight</span>
      
    
  
  </div>

  <div id="nijkamp2020learninh" class="col-sm-8">
    
      <div class="title">Learning Multi-layer Latent Variable Model via Variational Optimization of Short Run MCMC for Approximate Inference</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                
                  Zhou, Linqi,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>European Conference on Computer Vision (ECCV)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1912.01909" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/enijkamp/short_run_inf" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://enijkamp.github.io/project_short_run_inference/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="nijkamp2019anatomy" class="col-sm-8">
    
      <div class="title">On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1903.12370.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/point0bar1/ebm-anatomy" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://github.com/enijkamp/project_anatomy/blob/master/ebm-anatomy-poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_anatomy/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling.
On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training.
On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
      
    
  
  </div>

  <div id="pang2020learning" class="col-sm-8">
    
      <div class="title">Learning Latent Space Energy-Based Prior Model</div>
      <div class="author">
        
          
            
              
                
                  Pang, Bo*,
                
              
            
          
        
          
            
              
                
                  Han, Tian*,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik*</em>,
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2006.08205.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/bpucla/latent-space-EBM-prior" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The generator model assumes that the observed example is generated by a low-dimensional latent vector via a top-down network, and the latent vector follows a simple and known prior distribution, such as uniform or Gaussian white noise distribution. While we can learn an expressive top-down network to map the prior distribution to the data distribution, we can also learn an expressive prior model instead of assuming a given prior distribution. This follows the philosophy of empirical Bayes where the prior model is learned from the observed data. We propose to learn an energy-based prior model for the latent vector, where the energy function is parametrized by a very simple multi-layer perceptron. Due to the low-dimensionality of the latent space, learning a latent space energy-based prior model proves to be both feasible and desirable. In this paper, we develop the maximum likelihood learning algorithm and its variation based on short-run Markov chain Monte Carlo sampling from the prior and the posterior distributions of the latent vector, and we show that the learned model exhibits strong performance in terms of image and text generation and anomaly detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="gao2020flow" class="col-sm-8">
    
      <div class="title">Flow Contrastive Estimation of Energy-Based Models</div>
      <div class="author">
        
          
            
              
                
                  Gao, Ruiqi,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Kingma, Diederik P,
                
              
            
          
        
          
            
              
                
                  Xu, Zhen,
                
              
            
          
        
          
            
              
                
                  Dai, Andrew M,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1912.00589.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      <a href="http://www.stat.ucla.edu/ ruiqigao/fce/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
    
  
  </div>

  <div id="han2020joint" class="col-sm-8">
    
      <div class="title">Joint Training of Variational Auto-Encoder and Latent Energy-Based Model</div>
      <div class="author">
        
          
            
              
                
                  Han, Tian,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Zhou, Linqi,
                
              
            
          
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1812.10907.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/alexzhou907/vae_ebm" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://hthth0801.github.io/jointLearning/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
      
    
  
  </div>

  <div id="pang2020towards" class="col-sm-8">
    
      <div class="title">Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation</div>
      <div class="author">
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Han, Wenjuan,
                
              
            
          
        
          
            
              
                
                  Zhou, Linqi,
                
              
            
          
        
          
            
              
                
                  Liu, Yixian,
                
              
            
          
        
          
            
              
                
                  and Tu, Kewei
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="xie2020representation" class="col-sm-8">
    
      <div class="title">Representation Learning: A Statistical Perspective</div>
      <div class="author">
        
          
            
              
                
                  Xie, Jianwen,
                
              
            
          
        
          
            
              
                
                  Gao, Ruiqi,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Annual Review of Statistics and Its Application</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1911.11374" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="pang2020deep" class="col-sm-8">
    
      <div class="title">Deep Learning With TensorFlow: A Review</div>
      <div class="author">
        
          
            
              
                
                  Pang, Bo,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Educational and Behavioral Statistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://journals.sagepub.com/doi/pdf/10.3102/1076998619872761" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
      <span class="award badge">Oral</span>
      
    
  
  </div>

  <div id="han2019divergence" class="col-sm-8">
    
      <div class="title">Divergence Triangle for Joint Training of Generator model, Energy-based model, and Inferential model</div>
      <div class="author">
        
          
            
              
                
                  Han, Tian*,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik*</em>,
              
            
          
        
          
            
              
                
                  Fang, Xiaolin,
                
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1812.10907.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="http://github.com/enijkamp/triangle" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://raw.githubusercontent.com/enijkamp/project_triangle/master/cvpr2019_tianhan.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_triangle/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes the divergence triangle as a framework for joint training of generator model, energy-based model and inference model. The divergence triangle is a compact and symmetric (anti-symmetric) objective function that seamlessly integrates variational learning, adversarial learning, wake-sleep algorithm, and contrastive divergence in a unified probabilistic formulation. This unification makes the processes of sampling, inference, energy evaluation readily available without the need for costly Markov chain Monte Carlo methods. Our experiments demonstrate that the divergence triangle is capable of learning (1) an energy-based model with well-formed energy landscape, (2) direct sampling in the form of a generator network, and (3) feed-forward inference that faithfully reconstructs observed as well as synthesized data. The divergence triangle is a robust training method that can learn from incomplete data.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
      
    
  
  </div>

  <div id="nijkamp2019learning" class="col-sm-8">
    
      <div class="title">Learning Non-convergent Non-persistent Short-run MCMC toward Energy-Based Model</div>
      <div class="author">
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                
                  Zhu, Song-Chun,
                
              
            
          
        
          
            
              
                
                  and Wu, Ying Nian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1904.09770" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/enijkamp/short_run" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="https://github.com/enijkamp/project_short_run/blob/master/poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://enijkamp.github.io/project_short_run" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies a curious phenomenon in learning energy-based model (EBM) using MCMC. In each learning iteration, we generate synthesized examples by running a non-convergent, non-mixing, and non-persistent short-run MCMC toward the current model, always starting from the same initial distribution such as uniform noise distribution, and always running a fixed number of MCMC steps. After generating synthesized examples, we then update the model parameters according to the maximum likelihood learning gradient, as if the synthesized examples are fair samples from the current model. We treat this non-convergent short-run MCMC as a learned generator model or a flow model. We provide arguments for treating the learned non-convergent short-run MCMC as a valid model. We show that the learned short-run MCMC is capable of generating realistic images. More interestingly, unlike traditional EBM or MCMC, the learned short-run MCMC is capable of reconstructing observed images and interpolating between images, like generator or flow models. The code can be found in the Appendix.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">QAM</abbr>
      
    
  
  </div>

  <div id="hill2019building" class="col-sm-8">
    
      <div class="title">Building a telescope to look into high-dimensional image spaces</div>
      <div class="author">
        
          
            
              
                
                  Hill, Mitch,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  and Zhu, Song-Chun
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Quarterly of Applied Mathematics</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1803.01043" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>An image pattern can be represented by a probability distribution whose density is concentrated on different low-dimensional subspaces in the high-dimensional image space. Such probability densities have an astronomical number of local modes corresponding to typical pattern appearances. Related groups of modes can join to form macroscopic image basins that represent pattern concepts. Recent works use neural networks that capture high-order image statistics to learn Gibbs models capable of synthesizing realistic images of many patterns. However, characterizing a learned probability density to uncover the Hopfield memories of the model, encoded by the structure of the local modes, remains an open challenge. In this work, we present novel computational experiments that map and visualize the local mode structure of Gibbs densities. Efficient mapping requires identifying the global basins without enumerating the countless modes. Inspired by Grenander’s jump-diffusion method, we propose a new MCMC tool called Attraction-Diffusion (AD) that can capture the macroscopic structure of highly non-convex densities by measuring metastability of local modes. AD involves altering the target density with a magnetization potential penalizing distance from a known mode and running an MCMC sample of the altered density to measure the stability of the initial chain state. Using a low-dimensional generator network to facilitate exploration, we map image spaces with up to 12,288 dimensions (64 × 64 pixels in RGB). Our work shows: (1) AD can efficiently map highly non-convex probability densities, (2) metastable regions of pattern probability densities contain coherent groups of images, and (3) the perceptibility of differences between training images influences the metastability of image basins.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
      
    
  
  </div>

  <div id="deutsch2019generative" class="col-sm-8">
    
      <div class="title">A generative model for sampling high-performance and diverse weights for neural networks</div>
      <div class="author">
        
          
            
              
                
                  Deutsch, Lior,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  and Yang, Yu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:1905.02898</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1905.02898" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent work on mode connectivity in the loss landscape of deep neural networks has demonstrated that the locus of (sub-)optimal weight vectors lies on continuous paths. In this work, we train a neural network that serves as a hypernetwork, mapping a latent vector into high-performance (low-loss) weight vectors, generalizing recent findings of mode connectivity to higher dimensional manifolds. We formulate the training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We demonstrate how to reduce the number of parameters in the hypernetwork by parameter sharing. Once learned, the hypernetwork allows for a computationally efficient, ancestral sampling of neural network weights, which we recruit to form large ensembles. The improvement in classification accuracy obtained by this ensembling indicates that the generated manifold extends in dimensions other than directions implied by trivial symmetries. For computational efficiency, we distill an ensemble into a single classifier while retaining generalization.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
      
    
  
  </div>

  <div id="xie2017generative" class="col-sm-8">
    
      <div class="title">Generative hierarchical learning of sparse FRAME models</div>
      <div class="author">
        
          
            
              
                
                  Xie, Jianwen,
                
              
            
          
        
          
            
              
                
                  Xu, Yifei,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Nian Wu, Ying,
                
              
            
          
        
          
            
              
                
                  and Zhu, Song-Chun
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SIGKDD</abbr>
      
    
  
  </div>

  <div id="gemulla2011large" class="col-sm-8">
    
      <div class="title">Large-scale matrix factorization with distributed stochastic gradient descent</div>
      <div class="author">
        
          
            
              
                
                  Gemulla, Rainer,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Haas, Peter J,
                
              
            
          
        
          
            
              
                
                  and Sismanis, Yannis
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</em>
      
      
        2011
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.1145/2020408.2020426" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BTW</abbr>
      
    
  
  </div>

  <div id="alexandrov2011mapreduce" class="col-sm-8">
    
      <div class="title">MapReduce and PACT-comparing data parallel programming models</div>
      <div class="author">
        
          
            
              
                
                  Alexandrov, Alexander,
                
              
            
          
        
          
            
              
                
                  Ewen, Stephan,
                
              
            
          
        
          
            
              
                
                  Heimel, Max,
                
              
            
          
        
          
            
              
                
                  Hueske, Fabian,
                
              
            
          
        
          
            
              
                
                  Kao, Odej,
                
              
            
          
        
          
            
              
                
                  Markl, Volker,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  and Warneke, Daniel
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Datenbanksysteme für Business, Technologie und Web (BTW)</em>
      
      
        2011
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="http://stratosphere.eu/assets/papers/ComparingMapReduceAndPACTs_11.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hwang2010binrank" class="col-sm-8">
    
      <div class="title">Binrank: Scaling dynamic authority-based search using materialized subgraphs</div>
      <div class="author">
        
          
            
              
                
                  Hwang, Heasoo,
                
              
            
          
        
          
            
              
                
                  Balmin, Andrey,
                
              
            
          
        
          
            
              
                
                  Reinwald, Berthold,
                
              
            
          
        
          
            
              
                and <em>Nijkamp, Erik</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Knowledge and Data Engineering</em>
      
      
        2010
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">VLDB</abbr>
      
    
  
  </div>

  <div id="alexandrov2010massively" class="col-sm-8">
    
      <div class="title">Massively parallel data analysis with pacts on nephele</div>
      <div class="author">
        
          
            
              
                
                  Alexandrov, Alexander,
                
              
            
          
        
          
            
              
                
                  Heimel, Max,
                
              
            
          
        
          
            
              
                
                  Markl, Volker,
                
              
            
          
        
          
            
              
                
                  Battré, Dominic,
                
              
            
          
        
          
            
              
                
                  Hueske, Fabian,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Ewen, Stephan,
                
              
            
          
        
          
            
              
                
                  Kao, Odej,
                
              
            
          
        
          
            
              
                
                  and Warneke, Daniel
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the VLDB Endowment</em>
      
      
        2010
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.14778/1920841.1921056" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2008</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">VLDB</abbr>
      
    
  
  </div>

  <div id="baid2008dbpubs" class="col-sm-8">
    
      <div class="title">Dbpubs: multidimensional exploration of database publications</div>
      <div class="author">
        
          
            
              
                
                  Baid, Akanksha,
                
              
            
          
        
          
            
              
                
                  Balmin, Andrey,
                
              
            
          
        
          
            
              
                
                  Hwang, Heasoo,
                
              
            
          
        
          
            
              
                <em>Nijkamp, Erik</em>,
              
            
          
        
          
            
              
                
                  Rao, Jun,
                
              
            
          
        
          
            
              
                
                  Reinwald, Berthold,
                
              
            
          
        
          
            
              
                
                  Simitsis, Alkis,
                
              
            
          
        
          
            
              
                
                  Sismanis, Yannis,
                
              
            
          
        
          
            
              
                
                  and Ham, Frank
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the VLDB Endowment</em>
      
      
        2008
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-180825462-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-180825462-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
